{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385fdcfe-f980-455d-b31c-bcc656470cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc66cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\bert-entity-extraction\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "TOKENIZER = transformers.XLMRobertaTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"input/model/xlm-roberta-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208e7eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 62, 79315, 4, 70, 3299, 3447, 111, 10, 4589, 42, 122009, 3299, 27980, 297, 46132, 10, 3299, 93392, 98, 4571, 36659, 40404, 5, 587, 5, 62, 60875, 47, 10, 14364, 53, 70541, 214, 190219, 1294, 102158, 56065, 390, 70, 47, 1098, 52825, 4, 100, 110527, 57, 4126, 17368, 70, 30098, 454, 56359, 42, 122009, 132, 16, 55300, 4, 28, 5, 177, 5, 4, 6, 5, 64, 1176, 454, 51734, 454, 80581, 30675, 64, 2]\n",
      "<s> A string, the model id of a pretrained model hosted inside a model repo on huggingface.co. A path to a directory containing vocabulary files required by the tokenizer, for instance saved using the save_pretrained() method, e.g.,./my_model_directory/</s>\n",
      "['<s>', '▁A', '▁string', ',', '▁the', '▁model', '▁id', '▁of', '▁a', '▁pret', 'r', 'ained', '▁model', '▁host', 'ed', '▁inside', '▁a', '▁model', '▁repo', '▁on', '▁hu', 'gging', 'face', '.', 'co', '.', '▁A', '▁path', '▁to', '▁a', '▁director', 'y', '▁contain', 'ing', '▁vocabula', 'ry', '▁files', '▁required', '▁by', '▁the', '▁to', 'ken', 'izer', ',', '▁for', '▁instance', '▁sa', 'ved', '▁using', '▁the', '▁save', '_', 'pret', 'r', 'ained', '(', ')', '▁method', ',', '▁e', '.', 'g', '.', ',', '▁', '.', '/', 'my', '_', 'model', '_', 'direct', 'ory', '/', '</s>']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'XLMRobertaTokenizer' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\NLP\\bert-entity-extraction\\test.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP/bert-entity-extraction/test.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(TOKENIZER\u001b[39m.\u001b[39mdecode(s1))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP/bert-entity-extraction/test.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(TOKENIZER\u001b[39m.\u001b[39mconvert_ids_to_tokens(s1))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NLP/bert-entity-extraction/test.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(TOKENIZER\u001b[39m.\u001b[39;49mvocab())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'XLMRobertaTokenizer' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "s1 = TOKENIZER.encode(\"A string, the model id of a pretrained model hosted inside a model repo on huggingface.co. A path to a directory containing vocabulary files required by the tokenizer, for instance saved using the save_pretrained() method, e.g., ./my_model_directory/\", add_special_tokens=True)\n",
    "print(s1)\n",
    "print(TOKENIZER.decode(s1))\n",
    "print(TOKENIZER.convert_ids_to_tokens(s1))\n",
    "print(TOKENIZER.get_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dbc30d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "transformers.BertModel.from_pretrained(\"input/model/bert-base-uncased\", return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_test-2.txt\n",
      "d:\\NLP\\bert-entity-extraction\\input\\multiconer1_2022\\EN-English\n",
      "d:\\NLP\\bert-entity-extraction\\input\\multiconer1_2022\\EN-English\\en_test-2.conll\n"
     ]
    }
   ],
   "source": [
    "## first file in current dir (with full path)\n",
    "print(os.path.basename(\"input/multiconer1_2022/EN-English/en_test-2.conll\").split(\".\")[0] + \".txt\")\n",
    "print(os.path.dirname(os.path.abspath(\"input/multiconer1_2022/EN-English/en_test-2.conll\")))\n",
    "print(os.path.abspath(\"input/multiconer1_2022/EN-English/en_test-2.conll\"))\n",
    "# file = os.path.join(os.getcwd(), os.listdir(os.getcwd())[0])\n",
    "# file\n",
    "# os.path.dirname(file) ## directory of file\n",
    "# os.path.dirname(os.path.dirname(file)) ## directory of directory of file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc4d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1]), tensor([2]), tensor([3])]\n",
      "[1, 2, 3]\n",
      "[tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "tensor([2])\n",
      "tensor([2, 3])\n",
      "tensor([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = [torch.tensor([1]), torch.tensor([2]), torch.tensor([3])]\n",
    "print(x)\n",
    "x = [tensor.item() for tensor in x]\n",
    "print(x)\n",
    "a = [torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])] * 3\n",
    "print(a)\n",
    "for i, z in enumerate(zip(a, x)):\n",
    "    print(a[i][1:x[i]+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d464d6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\bert-entity-extraction\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"input/model/bert-base-uncased\",\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ee4cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '.', 'i', 'am', 'a', 'student', 'at', 'u', '##et', '.']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\NLP\\bert-entity-extraction\\test.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP/bert-entity-extraction/test.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokenized_sequence \u001b[39m=\u001b[39m TOKENIZER\u001b[39m.\u001b[39mtokenize(sentence)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP/bert-entity-extraction/test.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenized_sequence)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NLP/bert-entity-extraction/test.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenized_sequence[\u001b[39m\"\u001b[39;49m\u001b[39mtoken_type_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP/bert-entity-extraction/test.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(TOKENIZER(sentence)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP/bert-entity-extraction/test.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m sentence \u001b[39m=\u001b[39m TOKENIZER\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mHello.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mI am a student at UET.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "sentence = 'Hello. I am a student at UET.'\n",
    "# print(len(sentence.split()))\n",
    "tokenized_sequence = TOKENIZER.tokenize(sentence)\n",
    "print(tokenized_sequence)\n",
    "print(tokenized_sequence[\"token_type_ids\"])\n",
    "\n",
    "print(TOKENIZER(sentence)['input_ids'])\n",
    "sentence = TOKENIZER.encode(\"Hello.\", \"I am a student at UET.\")\n",
    "print(TOKENIZER.convert_ids_to_tokens(sentence))\n",
    "print(sentence)\n",
    "print(len(sentence))\n",
    "sentence = TOKENIZER.decode(sentence)\n",
    "print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.50      1.00      0.67         1\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.56      0.49         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4d93a-5d04-46cf-a5fe-e3339562f227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_20016\\3054647275.py:2: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_csv.loc[:, \"Sentence #\"] = df_csv[\"Sentence #\"].fillna(method=\"ffill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence: Sentence: 47959\n",
      "Train len: 43163\n",
      "Val len: 4796\n"
     ]
    }
   ],
   "source": [
    "df_csv = pd.read_csv(\"input/ner_dataset.csv\", encoding=\"latin-1\", keep_default_na=False, na_values=[''])\n",
    "df_csv.loc[:, \"Sentence #\"] = df_csv[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "sentences = df_csv.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "print(\"Total sentence: \" + df_csv[\"Sentence #\"].iloc[-1])\n",
    "(\n",
    "    train_sentences,\n",
    "    test_sentences,\n",
    ") = model_selection.train_test_split(sentences, random_state=42, test_size=0.1)\n",
    "\n",
    "print(\"Train len: \"+ str(len(train_sentences)))\n",
    "print(\"Val len: \"+ str(len(test_sentences)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159d1ce-1f7e-45df-be11-1c7cdb58eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences_into_df(path): #load into a list of list to create df\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    #sentence = [] #sentence: a list of line.split() for an actual sentence sample\n",
    "    sentence_cnt = 0\n",
    "    with open(path, encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            #print(line)\n",
    "            line = line.strip()\n",
    "            if not line: #empty line to separate two sentences\n",
    "                sentence_cnt += 1\n",
    "            else: \n",
    "                line = line.split()\n",
    "                sentence_word = [sentence_cnt]\n",
    "                if len(line) != 4 or line[1] != \"_\":\n",
    "                    #print(line)\n",
    "                    continue\n",
    "                else:\n",
    "                    sentence_word.extend([line[0], line[-1]])\n",
    "                    sentences.append(sentence_word)\n",
    "    return pd.DataFrame(sentences, columns = [\"Sentence #\", \"Word\", \"Tag\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff7012-540f-475b-b326-fd4de79f9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dev = load_sentences_into_df(\"input/multiconer1_2022/EN-English/en_dev.conll\")\n",
    "train_df = load_sentences_into_df(\"input/multiconer1_2022/EN-English/en_train.conll\")\n",
    "test_df = load_sentences_into_df(\"input/multiconer1_2022/EN-English/en_test.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c2fa8-0aa3-415c-b0ba-f3feed6624c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-CW B-PER B-CORP B-GRP B-LOC B-PROD\n",
      "['the', 'bbc', 'did', 'a', 'news', 'story', ',', 'which', 'talked', 'about', 'how', 'the', 'causes', 'of', 'the', 'crisis', 'in', 'the', 'u.s.', 'economy', 'has', 'forced', 'many', 'people', ',', 'who', 'used', 'to', 'own', 'their', 'own', 'homes', ',', 'to', 'now', 'live', 'in', 'tents', '.']\n",
      "39\n",
      "train set\n",
      "['in', '1981', ',', 'as', 'he', 'was', 'standing', 'in', 'mitel', '’s', 'lobby', 'with', 'one', 'of', 'the', 'company', '’s', '3', 'inch', 'wafers', 'in', 'his', 'hands', ',', 'light', 'hit', 'the', 'wafer', 'a', 'certain', 'way', 'and', 'he', 'saw', 'a', 'riot', 'of', 'colour', 'coming', 'off', '.']\n",
      "41\n",
      "test set\n",
      "['aside', 'from', 'the', 'sd', '.', 'kfz', '.', '4', '/', '1', ',', 'the', 'sd', '.', 'kfz', '.', '4', 'was', 'armed', 'only', 'with', 'a', 'light', '7.92', 'mm', 'mg', '34', 'or', 'mg', '42', 'machine', 'gun', 'with', 'a', 'traverse', 'of', '270', '°', 'and', 'elevation', 'limits', 'of', '12', '°', 'to', '+', '80', '°', '.']\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(filter(lambda x: \"B-\" in x, sentences_dev['Tag'].unique())))\n",
    "sentences = sentences_dev.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "print(max(sentences, key=len))\n",
    "longest = max(sentences, key=len)\n",
    "print(len(longest))\n",
    "\n",
    "print(\"train set\")\n",
    "sentences = train_df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "longest = max(sentences, key=len)\n",
    "print(longest)\n",
    "print(len(longest))\n",
    "\n",
    "print(\"test set\")\n",
    "sentences = test_df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "longest = max(sentences, key=len)\n",
    "print(longest)\n",
    "print(len(longest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68b96d-46b8-4fcc-9775-20de74384eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_df(df):\n",
    "    print(\"-- Total sentences: \" + str(df['Sentence #'].iloc[-1] + 1))\n",
    "    #print(\"-- Set of Tags: \" + \" \".join(filter(lambda x: \"B-\" in x, df['Tag'].unique())))\n",
    "    print(\"-- Set of Tags: \" + \" \".join(df['Tag'].unique()) + \" -> \" +str(df['Tag'].nunique()))\n",
    "    print(\"-- PER (person): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-PER\")].count()))\n",
    "    print(\"-- LOC (location): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-LOC\")].count()))\n",
    "    print(\"-- CORP (corporation): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-CORP\")].count()))\n",
    "    print(\"-- GRP (group): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-GRP\")].count()))\n",
    "    print(\"-- PROD (product): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-PROD\")].count()))\n",
    "    print(\"-- CW (creative work): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-CW\")].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b26f71-5461-4cea-a93c-e50f798522fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "-- Total sentences: 15300\n",
      "-- Set of Tags: O B-PER I-PER B-GRP I-GRP B-CW I-CW B-LOC B-CORP I-CORP B-PROD I-LOC I-PROD -> 13\n",
      "-- PER (person): 5397\n",
      "-- LOC (location): 4799\n",
      "-- CORP (corporation): 3111\n",
      "-- GRP (group): 3571\n",
      "-- PROD (product): 2923\n",
      "-- CW (creative work): 3752\n",
      "Validation set\n",
      "-- Total sentences: 800\n",
      "-- Set of Tags: B-CW I-CW O B-PER I-PER B-CORP I-CORP B-GRP I-GRP B-LOC I-LOC B-PROD I-PROD -> 13\n",
      "-- PER (person): 290\n",
      "-- LOC (location): 234\n",
      "-- CORP (corporation): 193\n",
      "-- GRP (group): 190\n",
      "-- PROD (product): 147\n",
      "-- CW (creative work): 176\n",
      "Test set\n",
      "-- Total sentences: 217818\n",
      "-- Set of Tags: B-PROD I-PROD O B-GRP B-LOC I-LOC I-GRP B-CORP I-CORP B-CW B-PER I-PER I-CW -> 13\n",
      "-- PER (person): 55682\n",
      "-- LOC (location): 59082\n",
      "-- CORP (corporation): 37435\n",
      "-- GRP (group): 41156\n",
      "-- PROD (product): 36786\n",
      "-- CW (creative work): 42781\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set\")\n",
    "describe_df(df_train)\n",
    "print(\"Validation set\")\n",
    "describe_df(df_dev)\n",
    "print(\"Test set\")\n",
    "describe_df(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
