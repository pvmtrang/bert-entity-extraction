{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385fdcfe-f980-455d-b31c-bcc656470cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import osi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abc66cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLP\\bert-entity-extraction\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_test-2.txt\n",
      "d:\\NLP\\bert-entity-extraction\\input\\multiconer1_2022\\EN-English\n",
      "d:\\NLP\\bert-entity-extraction\\input\\multiconer1_2022\\EN-English\\en_test-2.conll\n"
     ]
    }
   ],
   "source": [
    "## first file in current dir (with full path)\n",
    "print(os.path.basename(\"input/multiconer1_2022/EN-English/en_test-2.conll\").split(\".\")[0] + \".txt\")\n",
    "print(os.path.dirname(os.path.abspath(\"input/multiconer1_2022/EN-English/en_test-2.conll\")))\n",
    "print(os.path.abspath(\"input/multiconer1_2022/EN-English/en_test-2.conll\"))\n",
    "# file = os.path.join(os.getcwd(), os.listdir(os.getcwd())[0])\n",
    "# file\n",
    "# os.path.dirname(file) ## directory of file\n",
    "# os.path.dirname(os.path.dirname(file)) ## directory of directory of file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc4d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1]), tensor([2]), tensor([3])]\n",
      "[1, 2, 3]\n",
      "[tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]\n",
      "tensor([2])\n",
      "tensor([2, 3])\n",
      "tensor([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = [torch.tensor([1]), torch.tensor([2]), torch.tensor([3])]\n",
    "print(x)\n",
    "x = [tensor.item() for tensor in x]\n",
    "print(x)\n",
    "a = [torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])] * 3\n",
    "print(a)\n",
    "for i, z in enumerate(zip(a, x)):\n",
    "    print(a[i][1:x[i]+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d464d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path = \"input/bert-base-uncased\",\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ee4cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "26\n",
      "[101, 2010, 2377, 9863, 2950, 13584, 21146, 18933, 3600, 1010, 1043, 4143, 1010, 2406, 27071, 2015, 1998, 1996, 12536, 1038, 1012, 1045, 1012, 1043, 1012, 102]\n",
      "['[CLS]', 'his', 'play', '##list', 'includes', 'sonny', 'sha', '##rro', '##ck', ',', 'g', '##za', ',', 'country', 'teaser', '##s', 'and', 'the', 'notorious', 'b', '.', 'i', '.', 'g', '.', '[SEP]']\n",
      "[CLS] his playlist includes sonny sharrock, gza, country teasers and the notorious b. i. g. [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'his playlist includes sonny sharrock, gza, country teasers and the notorious b. i. g.'\n",
    "print(len(sentence.split()))\n",
    "sentence = TOKENIZER.encode(sentence)\n",
    "print(len(sentence))\n",
    "print(sentence)\n",
    "print(TOKENIZER.convert_ids_to_tokens(sentence))\n",
    "sentence = TOKENIZER.decode(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.50      1.00      0.67         1\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.56      0.49         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4d93a-5d04-46cf-a5fe-e3339562f227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_20016\\3054647275.py:2: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_csv.loc[:, \"Sentence #\"] = df_csv[\"Sentence #\"].fillna(method=\"ffill\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence: Sentence: 47959\n",
      "Train len: 43163\n",
      "Val len: 4796\n"
     ]
    }
   ],
   "source": [
    "df_csv = pd.read_csv(\"input/ner_dataset.csv\", encoding=\"latin-1\", keep_default_na=False, na_values=[''])\n",
    "df_csv.loc[:, \"Sentence #\"] = df_csv[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "sentences = df_csv.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "print(\"Total sentence: \" + df_csv[\"Sentence #\"].iloc[-1])\n",
    "(\n",
    "    train_sentences,\n",
    "    test_sentences,\n",
    ") = model_selection.train_test_split(sentences, random_state=42, test_size=0.1)\n",
    "\n",
    "print(\"Train len: \"+ str(len(train_sentences)))\n",
    "print(\"Val len: \"+ str(len(test_sentences)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159d1ce-1f7e-45df-be11-1c7cdb58eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences_into_df(path): #load into a list of list to create df\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    #sentence = [] #sentence: a list of line.split() for an actual sentence sample\n",
    "    sentence_cnt = 0\n",
    "    with open(path, encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            #print(line)\n",
    "            line = line.strip()\n",
    "            if not line: #empty line to separate two sentences\n",
    "                sentence_cnt += 1\n",
    "            else: \n",
    "                line = line.split()\n",
    "                sentence_word = [sentence_cnt]\n",
    "                if len(line) != 4 or line[1] != \"_\":\n",
    "                    #print(line)\n",
    "                    continue\n",
    "                else:\n",
    "                    sentence_word.extend([line[0], line[-1]])\n",
    "                    sentences.append(sentence_word)\n",
    "    return pd.DataFrame(sentences, columns = [\"Sentence #\", \"Word\", \"Tag\"])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff7012-540f-475b-b326-fd4de79f9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dev = load_sentences_into_df(\"input/multiconer1_2022/EN-English/en_dev.conll\")\n",
    "train_df = load_sentences_into_df(\"input/multiconer1_2022/EN-English/en_train.conll\")\n",
    "test_df = load_sentences_into_df(\"input/multiconer1_2022/EN-English/en_test.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c2fa8-0aa3-415c-b0ba-f3feed6624c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-CW B-PER B-CORP B-GRP B-LOC B-PROD\n",
      "['the', 'bbc', 'did', 'a', 'news', 'story', ',', 'which', 'talked', 'about', 'how', 'the', 'causes', 'of', 'the', 'crisis', 'in', 'the', 'u.s.', 'economy', 'has', 'forced', 'many', 'people', ',', 'who', 'used', 'to', 'own', 'their', 'own', 'homes', ',', 'to', 'now', 'live', 'in', 'tents', '.']\n",
      "39\n",
      "train set\n",
      "['in', '1981', ',', 'as', 'he', 'was', 'standing', 'in', 'mitel', '’s', 'lobby', 'with', 'one', 'of', 'the', 'company', '’s', '3', 'inch', 'wafers', 'in', 'his', 'hands', ',', 'light', 'hit', 'the', 'wafer', 'a', 'certain', 'way', 'and', 'he', 'saw', 'a', 'riot', 'of', 'colour', 'coming', 'off', '.']\n",
      "41\n",
      "test set\n",
      "['aside', 'from', 'the', 'sd', '.', 'kfz', '.', '4', '/', '1', ',', 'the', 'sd', '.', 'kfz', '.', '4', 'was', 'armed', 'only', 'with', 'a', 'light', '7.92', 'mm', 'mg', '34', 'or', 'mg', '42', 'machine', 'gun', 'with', 'a', 'traverse', 'of', '270', '°', 'and', 'elevation', 'limits', 'of', '12', '°', 'to', '+', '80', '°', '.']\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(filter(lambda x: \"B-\" in x, sentences_dev['Tag'].unique())))\n",
    "sentences = sentences_dev.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "print(max(sentences, key=len))\n",
    "longest = max(sentences, key=len)\n",
    "print(len(longest))\n",
    "\n",
    "print(\"train set\")\n",
    "sentences = train_df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "longest = max(sentences, key=len)\n",
    "print(longest)\n",
    "print(len(longest))\n",
    "\n",
    "print(\"test set\")\n",
    "sentences = test_df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "longest = max(sentences, key=len)\n",
    "print(longest)\n",
    "print(len(longest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68b96d-46b8-4fcc-9775-20de74384eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_df(df):\n",
    "    print(\"-- Total sentences: \" + str(df['Sentence #'].iloc[-1] + 1))\n",
    "    #print(\"-- Set of Tags: \" + \" \".join(filter(lambda x: \"B-\" in x, df['Tag'].unique())))\n",
    "    print(\"-- Set of Tags: \" + \" \".join(df['Tag'].unique()) + \" -> \" +str(df['Tag'].nunique()))\n",
    "    print(\"-- PER (person): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-PER\")].count()))\n",
    "    print(\"-- LOC (location): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-LOC\")].count()))\n",
    "    print(\"-- CORP (corporation): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-CORP\")].count()))\n",
    "    print(\"-- GRP (group): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-GRP\")].count()))\n",
    "    print(\"-- PROD (product): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-PROD\")].count()))\n",
    "    print(\"-- CW (creative work): \" + str(df['Tag'].loc[df['Tag'].str.contains(\"B-CW\")].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b26f71-5461-4cea-a93c-e50f798522fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "-- Total sentences: 15300\n",
      "-- Set of Tags: O B-PER I-PER B-GRP I-GRP B-CW I-CW B-LOC B-CORP I-CORP B-PROD I-LOC I-PROD -> 13\n",
      "-- PER (person): 5397\n",
      "-- LOC (location): 4799\n",
      "-- CORP (corporation): 3111\n",
      "-- GRP (group): 3571\n",
      "-- PROD (product): 2923\n",
      "-- CW (creative work): 3752\n",
      "Validation set\n",
      "-- Total sentences: 800\n",
      "-- Set of Tags: B-CW I-CW O B-PER I-PER B-CORP I-CORP B-GRP I-GRP B-LOC I-LOC B-PROD I-PROD -> 13\n",
      "-- PER (person): 290\n",
      "-- LOC (location): 234\n",
      "-- CORP (corporation): 193\n",
      "-- GRP (group): 190\n",
      "-- PROD (product): 147\n",
      "-- CW (creative work): 176\n",
      "Test set\n",
      "-- Total sentences: 217818\n",
      "-- Set of Tags: B-PROD I-PROD O B-GRP B-LOC I-LOC I-GRP B-CORP I-CORP B-CW B-PER I-PER I-CW -> 13\n",
      "-- PER (person): 55682\n",
      "-- LOC (location): 59082\n",
      "-- CORP (corporation): 37435\n",
      "-- GRP (group): 41156\n",
      "-- PROD (product): 36786\n",
      "-- CW (creative work): 42781\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set\")\n",
    "describe_df(df_train)\n",
    "print(\"Validation set\")\n",
    "describe_df(df_dev)\n",
    "print(\"Test set\")\n",
    "describe_df(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
